# K-邻近算法(k-Nearest Neighbor)
[toc]
## 原理  
计算测试数据与训练数据集中数据的距离，选取K个距离最短的数据，对选取的数据按照标签分类，按频率最高的标签来作为测试数据的分类.

## 步骤
1. 准备训练样本集.
2. 计算测试数据与训练样本集中样本的距离.
3. 选取K(一般小于20)个距离最小的样本.
4. 在选取的样本中，按照频率最高的样本标签对测试数据进行分类.  

## 特点
* 优点：精度高，对异常值不敏感，无数据输入假定
* 缺点：时间和空间复杂度较高
* 适用范围：数值型

## 距离
* 欧氏(Euclidean)距离 (k表示维数，下同)
	> `$L_(x, x_m) = \sqrt{\sum_{1}^{n}{(x^k - x_m^k )^2}}$`
	* 曼哈顿(Manhattan)距离  
	> `$L_(x, x_m) = \sum_{1}^{n}|x^k - x_m^k|$`

### 归一化
	为了消除各个维度上距离数量级的差异，将各个维度的数据映射到的[0, 1]范围内.通常做法为将距离差比上距离的范围，则欧氏距离可以转化为：
	> `$L_(x, x_m) = \sqrt{\sum_{1}^{n}{(\frac{x^k - x_m^k}{x_{max}^k-x_{min}^k}})^2}$`

### 加权
	为了体现每个维度对于结果的影响的大小，可以为每个维度添加一个权值, p表示权值
	> `$L_(x, x_m) = \sqrt{\sum_{1}^{n}{p^{k} \cdot (\frac{x^k - x_m^k}{x_{max}^k-x_{min}^k}})^2}$`
